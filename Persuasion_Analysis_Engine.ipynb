{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcKkU1ajhtD0mVkjZDF38x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FaarisIq/Persuasion-Analysis-Engine/blob/main/Persuasion_Analysis_Engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Persuasion Analysis Engine - Faaris Iqbal"
      ],
      "metadata": {
        "id": "reb5_ddN97rS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install praw spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install praw pandas spacy vaderSentiment\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "1Lj3tS3g63wk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "import json\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from IPython.display import display, HTML\n",
        "import time\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "\"\"\"\n",
        "Persuasion Analysis Engine - Faaris Iqbal\n",
        "\n",
        "This engine analyzes persuasiveness in the subreddit r/changemyview by\n",
        "collecting the top posts and then scoring them based off of different\n",
        "factors.\n",
        "\n",
        "Main Features Include:\n",
        "- Data collection of top 100 posts and their comments using Reddit API\n",
        "- Analyzing the argument's structure, evidence quality, and persuasive techniques\n",
        "- Detects deltas awarded (gift awarded to OP when one's view is changed)\n",
        "- Scores persuasiveness from 0-1 using weights\n",
        "- Exports clean datasets for further research\n",
        "\n",
        "The persuasion factors analyzed are:\n",
        "1. Argument Length/Depth (25% weight) - Detailed vs surface level arguments\n",
        "2. Evidence quality (20% weight) - Academic sources vs blogs\n",
        "3. Argument sophistication (20% weight) - Logical structure\n",
        "4. Delta Awards (15% weight) - Proof of persuasion\n",
        "5. Comment Engagement (10% weight) - Quality of responses and discussion\n",
        "6) Emotional appeal (10% weight) - Emotional connection and language used\n",
        "\n",
        "The persuasive techniques detected are:\n",
        "- Analogies\n",
        "- Rhetorical questions and direct questions\n",
        "- Stats and data\n",
        "- Personal experience\n",
        "- Moral/ethical appeals\n",
        "- Authority citations\n",
        "- Concessions\n",
        "- Counterargument acknowledgement\n",
        "- Logical connectors\n",
        "\n",
        "It outputs:\n",
        "- 'cmv_posts_analysis.csv' - Main post data with all metrics\n",
        "- 'cmv_comments_analysis.csv' - Individual comment analysis\n",
        "- 'cmv_summary_stats.csv' - Aggregated statistics\n",
        "- Summary stats of most/least persuasive posts\n",
        "\n",
        "To use it:\n",
        "You run the script and it automatically collects data and generates CSV files\n",
        "You can also adjust the limit parameter in collect_cmv_data() and in the main\n",
        "function at the bottom in order to change the amount of posts being analyzed\n",
        "\"\"\"\n",
        "\n",
        "# reddit API setup\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"1Mqp8_sUj6ivNylhouNiUg\",\n",
        "    client_secret=\"uKxoTRwLFpA8p_JKp-20tV4qbiWcoA\",\n",
        "    user_agent=\"changemyview_data_collector_v2\"\n",
        ")\n",
        "\n",
        "# nlp setup\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# scoring weights\n",
        "SCORING_WEIGHTS = {\n",
        "    'length': 0.25,\n",
        "    'evidence': 0.20,\n",
        "    'sophistication': 0.20,\n",
        "    'delta': 0.15,\n",
        "    'engagement': 0.10,\n",
        "    'emotion': 0.10\n",
        "}\n",
        "\n",
        "# text preprocessing\n",
        "def clean_text(text):\n",
        "    \"\"\"Enhanced text cleaning for Reddit content\"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove Reddit markdown\n",
        "    text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text)\n",
        "    text = re.sub(r'\\*(.*?)\\*', r'\\1', text)\n",
        "    text = re.sub(r'\\[([^\\]]+)\\]\\([^)]+\\)', r'\\1', text)\n",
        "    text = re.sub(r'&gt;.*?\\n', '', text)\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'\\n+', ' ', text)\n",
        "    text = re.sub(r'[^\\w\\s.,!?;:-]', '', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def spacy_sent_tokenize(text):\n",
        "    \"\"\"sentence tokenization\"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        doc = nlp(text)\n",
        "        return [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "    except:\n",
        "        # Fallback to simple splitting if spacy fails\n",
        "        return [s.strip() for s in text.split('.') if s.strip()]\n",
        "\n",
        "# delta detection (a delta is an award given to OP if the gifter was persuaded)\n",
        "def extract_actual_deltas(post_comments):\n",
        "    delta_count = 0\n",
        "    delta_patterns = [\n",
        "        r'!delta\\b',\n",
        "        r'∆',\n",
        "        r'Δ',\n",
        "        r'awarded.*?delta',\n",
        "        r'changed my view',\n",
        "        r'view.*?changed',\n",
        "        r'cmv.*?successful'\n",
        "    ]\n",
        "\n",
        "    for comment in post_comments:\n",
        "        body = comment.get('body', '').lower()\n",
        "\n",
        "        # Skip bot/moderator comments\n",
        "        if any(bot in str(comment.get('author', '')).lower()\n",
        "               for bot in ['deltabot', 'automoderator', '[deleted]']):\n",
        "            continue\n",
        "\n",
        "        # Check for delta indicators\n",
        "        for pattern in delta_patterns:\n",
        "            if re.search(pattern, body, re.I):\n",
        "                delta_count += 1\n",
        "                break  # Max one delta per comment\n",
        "\n",
        "    return delta_count\n",
        "\n",
        "def is_delta_metadata_comment(text):\n",
        "    \"\"\"Filter out system delta messages\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return False\n",
        "\n",
        "    metadata_phrases = [\n",
        "        \"all comments that earned deltas\",\n",
        "        \"delta system explained\",\n",
        "        \"/r/deltalog\",\n",
        "        \"change of view doesn't necessarily mean a reversal\",\n",
        "        \"awarded a delta\",\n",
        "        \"confirmation that a delta has been awarded\"\n",
        "    ]\n",
        "\n",
        "    lowered = text.lower()\n",
        "    return any(phrase in lowered for phrase in metadata_phrases)\n",
        "\n",
        "# evidence quality analysis\n",
        "def analyze_source_credibility(text):\n",
        "    urls = re.findall(r'https?://[^\\s<>\\[\\]]+', text)\n",
        "    if not urls:\n",
        "        return 0\n",
        "\n",
        "    credibility_score = 0\n",
        "\n",
        "    # High credibility sources\n",
        "    high_cred_domains = [\n",
        "        '.edu', '.gov', 'scholar.google', 'jstor', 'pubmed',\n",
        "        'nature.com', 'science.org', 'cell.com', 'nejm.org',\n",
        "        'stanford.edu', 'harvard.edu', 'mit.edu'\n",
        "    ]\n",
        "\n",
        "    # Medium credibility sources\n",
        "    medium_cred_domains = [\n",
        "        'reuters.com', 'ap.org', 'bbc.com', 'npr.org',\n",
        "        'economist.com', 'wsj.com', 'nytimes.com',\n",
        "        'washingtonpost.com', 'theguardian.com'\n",
        "    ]\n",
        "\n",
        "    # Low credibility indicators\n",
        "    low_cred_indicators = [\n",
        "        'blog', 'wordpress', 'medium.com', 'reddit.com',\n",
        "        'youtube.com', 'twitter.com', 'facebook.com'\n",
        "    ]\n",
        "\n",
        "    for url in urls:\n",
        "        url_lower = url.lower()\n",
        "\n",
        "        if any(domain in url_lower for domain in high_cred_domains):\n",
        "            credibility_score += 1.0\n",
        "        elif any(domain in url_lower for domain in medium_cred_domains):\n",
        "            credibility_score += 0.7\n",
        "        elif any(indicator in url_lower for indicator in low_cred_indicators):\n",
        "            credibility_score += 0.2\n",
        "        else:\n",
        "            credibility_score += 0.4  # Generic web source\n",
        "\n",
        "    return min(credibility_score / len(urls), 1.0)\n",
        "\n",
        "# argument sophistication analysis\n",
        "def detect_argument_sophistication(text):\n",
        "    \"\"\"Measure argument quality and sophistication\"\"\"\n",
        "    if not text:\n",
        "        return 0\n",
        "\n",
        "    text_lower = text.lower()\n",
        "    sophistication_score = 0\n",
        "\n",
        "    # Counterargument acknowledgment (high value)\n",
        "    counter_patterns = [\n",
        "        r\"some might argue\", r\"critics say\", r\"on the other hand\",\n",
        "        r\"however\", r\"nevertheless\", r\"although\", r\"admittedly\",\n",
        "        r\"while.*?true\", r\"granted\", r\"i understand.*?but\",\n",
        "        r\"fair point.*?but\", r\"you could argue\"\n",
        "    ]\n",
        "    counter_count = sum(1 for p in counter_patterns if re.search(p, text_lower))\n",
        "    sophistication_score += min(counter_count * 0.15, 0.4)\n",
        "\n",
        "    # Qualification/nuance\n",
        "    nuance_patterns = [\n",
        "        r\"in some cases\", r\"generally\", r\"tends to\", r\"often\",\n",
        "        r\"usually\", r\"primarily\", r\"largely\", r\"typically\",\n",
        "        r\"in most cases\", r\"under certain conditions\"\n",
        "    ]\n",
        "    nuance_count = sum(1 for p in nuance_patterns if re.search(p, text_lower))\n",
        "    sophistication_score += min(nuance_count * 0.08, 0.3)\n",
        "\n",
        "    # Evidence integration\n",
        "    evidence_patterns = [\n",
        "        r\"according to\", r\"research shows\", r\"studies indicate\",\n",
        "        r\"data suggests\", r\"evidence shows\", r\"surveys found\",\n",
        "        r\"analysis reveals\", r\"statistics show\"\n",
        "    ]\n",
        "    evidence_count = sum(1 for p in evidence_patterns if re.search(p, text_lower))\n",
        "    sophistication_score += min(evidence_count * 0.1, 0.3)\n",
        "\n",
        "    # Logical structure indicators\n",
        "    logic_patterns = [\n",
        "        r\"first\", r\"second\", r\"third\", r\"finally\",\n",
        "        r\"therefore\", r\"thus\", r\"consequently\", r\"as a result\",\n",
        "        r\"this leads to\", r\"it follows that\"\n",
        "    ]\n",
        "    logic_count = sum(1 for p in logic_patterns if re.search(p, text_lower))\n",
        "    sophistication_score += min(logic_count * 0.05, 0.2)\n",
        "\n",
        "    return min(sophistication_score, 1.0)\n",
        "\n",
        "# persuasive Features\n",
        "def analyze_persuasive_features(text):\n",
        "    \"\"\"Comprehensive persuasive element detection\"\"\"\n",
        "    if not text:\n",
        "        return {}\n",
        "\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    features = {\n",
        "        'analogies': len(re.findall(r'\\b(like|as if|similar to|just as|imagine if)\\b', text_lower)),\n",
        "        'questions': len(re.findall(r'\\?', text)),\n",
        "        'statistics': len(re.findall(r'\\b\\d+(\\.\\d+)?%?|\\bpercent\\b|\\bratio\\b|\\btimes\\b', text_lower)),\n",
        "        'hedging': len(re.findall(r'\\b(i think|maybe|possibly|could be|might be|seems like)\\b', text_lower)),\n",
        "        'personal_experience': len(re.findall(r'\\b(i have|i\\'ve|my experience|personally|i witnessed)\\b', text_lower)),\n",
        "        'moral_appeals': len(re.findall(r'\\b(moral|ethics|right|wrong|should|ought|duty|responsibility)\\b', text_lower)),\n",
        "        'emotional_appeals': len(re.findall(r'\\b(feel|emotion|heart|compassion|empathy|sympathy)\\b', text_lower)),\n",
        "        'authority_appeals': len(re.findall(r'\\b(expert|professor|doctor|researcher|authority|official)\\b', text_lower)),\n",
        "        'consensus_appeals': len(re.findall(r'\\b(everyone|most people|society|generally accepted|common sense)\\b', text_lower)),\n",
        "        'concessions': len(re.findall(r'\\b(i admit|you\\'re right|fair point|i concede|granted)\\b', text_lower))\n",
        "    }\n",
        "\n",
        "    return features\n",
        "\n",
        "def score_persuasive_features(features):\n",
        "    \"\"\"Convert feature counts to normalized score\"\"\"\n",
        "    if not features:\n",
        "        return 0\n",
        "\n",
        "    weights = {\n",
        "        'analogies': 0.15,\n",
        "        'questions': 0.10,\n",
        "        'statistics': 0.20,\n",
        "        'hedging': 0.05,\n",
        "        'personal_experience': 0.12,\n",
        "        'moral_appeals': 0.10,\n",
        "        'emotional_appeals': 0.08,\n",
        "        'authority_appeals': 0.15,\n",
        "        'consensus_appeals': 0.10,\n",
        "        'concessions': 0.15\n",
        "    }\n",
        "\n",
        "    score = 0\n",
        "    for feature, count in features.items():\n",
        "        if feature in weights:\n",
        "            normalized_count = min(count / 3, 1.0)\n",
        "            score += weights[feature] * normalized_count\n",
        "\n",
        "    return min(score, 1.0)\n",
        "\n",
        "# comment engagement analysis\n",
        "def analyze_comment_engagement(comments_data):\n",
        "    \"\"\"Analyze quality of community response\"\"\"\n",
        "    if not comments_data:\n",
        "        return 0\n",
        "\n",
        "    engagement_score = 0\n",
        "\n",
        "    # Average comment length (longer = more thoughtful)\n",
        "    avg_length = np.mean([len(c.get('body', '')) for c in comments_data])\n",
        "    length_score = min(avg_length / 500, 0.4)\n",
        "\n",
        "    # Comment depth (replies to replies = deeper engagement)\n",
        "    root_comments = [c for c in comments_data if c.get('is_root', False)]\n",
        "    non_root_comments = [c for c in comments_data if not c.get('is_root', False)]\n",
        "    depth_score = min(len(non_root_comments) / max(len(root_comments), 1) * 0.2, 0.3)\n",
        "\n",
        "    # Quality indicators in comments\n",
        "    quality_indicators = 0\n",
        "    for comment in comments_data[:10]:  # Check top 10 comments\n",
        "        body = comment.get('body', '').lower()\n",
        "\n",
        "        # Positive engagement\n",
        "        if any(word in body for word in ['interesting', 'good point', 'valid', 'thoughtful']):\n",
        "            quality_indicators += 0.1\n",
        "\n",
        "        # Constructive disagreement\n",
        "        if any(word in body for word in ['however', 'but consider', 'what about', 'counterpoint']):\n",
        "            quality_indicators += 0.15\n",
        "\n",
        "        # Evidence sharing\n",
        "        if any(word in body for word in ['source', 'link', 'study', 'research']):\n",
        "            quality_indicators += 0.1\n",
        "\n",
        "    engagement_score = length_score + depth_score + min(quality_indicators, 0.3)\n",
        "    return min(engagement_score, 1.0)\n",
        "\n",
        "# Sentiment Analysis\n",
        "def get_enhanced_emotion_scores(text):\n",
        "    \"\"\"Combine VADER with emotional analysis that is domain-based\"\"\"\n",
        "    vader_scores = analyzer.polarity_scores(text)\n",
        "\n",
        "    # CMV-specific emotional indicators\n",
        "    persuasive_emotions = [\n",
        "        'understand', 'realize', 'believe', 'feel', 'think',\n",
        "        'important', 'significant', 'crucial', 'essential'\n",
        "    ]\n",
        "\n",
        "    emotional_intensity = sum(1 for word in persuasive_emotions if word in text.lower())\n",
        "    emotional_score = min(emotional_intensity / 15, 0.5)  # Normalize\n",
        "\n",
        "    return {\n",
        "        'vader_compound': vader_scores['compound'],\n",
        "        'vader_pos': vader_scores['pos'],\n",
        "        'vader_neg': vader_scores['neg'],\n",
        "        'vader_neu': vader_scores['neu'],\n",
        "        'persuasive_emotion': emotional_score\n",
        "    }\n",
        "\n",
        "# persuasion scoring function\n",
        "def compute_enhanced_persuasion_score(post_text, comments_data, actual_deltas=0):\n",
        "    \"\"\"Comprehensive persuasion scoring with improved methodology\"\"\"\n",
        "\n",
        "    if not post_text:\n",
        "        return 0\n",
        "\n",
        "    # 1. Length/Depth Analysis\n",
        "    sentences = spacy_sent_tokenize(post_text)\n",
        "    length_score = min(len(sentences) / 25, 1)  # Optimal around 25 sentences\n",
        "\n",
        "    # 2. Evidence Quality\n",
        "    evidence_score = analyze_source_credibility(post_text)\n",
        "\n",
        "    # 3. Argument Sophistication\n",
        "    sophistication_score = detect_argument_sophistication(post_text)\n",
        "\n",
        "    # 4. Delta Integration (ground truth)\n",
        "    delta_score = min(actual_deltas * 0.25, 1.0)  # Each delta worth 0.25, cap at 1.0\n",
        "\n",
        "    # 5. Comment Engagement Quality\n",
        "    engagement_score = analyze_comment_engagement(comments_data)\n",
        "\n",
        "    # 6. Emotional Connection\n",
        "    emotion_data = get_enhanced_emotion_scores(post_text)\n",
        "    emotion_score = emotion_data['persuasive_emotion']\n",
        "\n",
        "    # Weighted final score\n",
        "    total_score = (\n",
        "        SCORING_WEIGHTS['length'] * length_score +\n",
        "        SCORING_WEIGHTS['evidence'] * evidence_score +\n",
        "        SCORING_WEIGHTS['sophistication'] * sophistication_score +\n",
        "        SCORING_WEIGHTS['delta'] * delta_score +\n",
        "        SCORING_WEIGHTS['engagement'] * engagement_score +\n",
        "        SCORING_WEIGHTS['emotion'] * emotion_score\n",
        "    )\n",
        "\n",
        "    return round(min(total_score, 1.0), 3)\n",
        "\n",
        "def format_timestamp(unix_timestamp):\n",
        "    \"\"\"Convert Unix timestamp to readable format\"\"\"\n",
        "    try:\n",
        "        return datetime.fromtimestamp(unix_timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "# data collection\n",
        "def collect_cmv_data(limit=3, time_filter=\"year\"):\n",
        "    print(f\" Starting enhanced CMV data collection (limit: {limit})\")\n",
        "\n",
        "    subreddit = reddit.subreddit(\"changemyview\")\n",
        "    posts_data = []\n",
        "    comments_data = []\n",
        "\n",
        "    for i, post in enumerate(subreddit.top(time_filter=time_filter, limit=limit)):\n",
        "        print(f\" Processing post {i+1}/{limit}: {post.title[:50]}...\")\n",
        "\n",
        "        # Get comments\n",
        "        post.comment_sort = 'top'\n",
        "        post.comments.replace_more(limit=5)\n",
        "\n",
        "        post_comments = []\n",
        "\n",
        "        for comment in post.comments.list():\n",
        "            if is_delta_metadata_comment(comment.body):\n",
        "                continue\n",
        "\n",
        "            cleaned_body = clean_text(comment.body)\n",
        "            if len(cleaned_body) < 10:  # Skip very short comments\n",
        "                continue\n",
        "\n",
        "            # Analyze individual comment\n",
        "            comment_features = analyze_persuasive_features(cleaned_body)\n",
        "            comment_emotion = get_enhanced_emotion_scores(cleaned_body)\n",
        "            comment_args = spacy_sent_tokenize(cleaned_body)\n",
        "\n",
        "            comment_data = {\n",
        "                'post_id': post.id,\n",
        "                'comment_id': comment.id,\n",
        "                'author': str(comment.author),\n",
        "                'comment_text': cleaned_body,\n",
        "                'comment_score': comment.score,\n",
        "                'created_timestamp': format_timestamp(comment.created_utc),\n",
        "                'created_utc': comment.created_utc,\n",
        "                'is_root_comment': comment.parent_id == post.id,\n",
        "                'comment_word_count': len(cleaned_body.split()),\n",
        "                'comment_sentence_count': len(comment_args),\n",
        "\n",
        "                # Persuasive features (flattened)\n",
        "                'analogies_count': comment_features.get('analogies', 0),\n",
        "                'questions_count': comment_features.get('questions', 0),\n",
        "                'statistics_count': comment_features.get('statistics', 0),\n",
        "                'hedging_count': comment_features.get('hedging', 0),\n",
        "                'personal_experience_count': comment_features.get('personal_experience', 0),\n",
        "                'moral_appeals_count': comment_features.get('moral_appeals', 0),\n",
        "                'emotional_appeals_count': comment_features.get('emotional_appeals', 0),\n",
        "                'authority_appeals_count': comment_features.get('authority_appeals', 0),\n",
        "                'consensus_appeals_count': comment_features.get('consensus_appeals', 0),\n",
        "                'concessions_count': comment_features.get('concessions', 0),\n",
        "\n",
        "                # Sentiment scores (flattened)\n",
        "                'vader_compound': comment_emotion['vader_compound'],\n",
        "                'vader_positive': comment_emotion['vader_pos'],\n",
        "                'vader_negative': comment_emotion['vader_neg'],\n",
        "                'vader_neutral': comment_emotion['vader_neu'],\n",
        "                'persuasive_emotion_score': comment_emotion['persuasive_emotion']\n",
        "            }\n",
        "\n",
        "            comments_data.append(comment_data)\n",
        "            post_comments.append({\n",
        "                'body': cleaned_body,\n",
        "                'author': str(comment.author),\n",
        "                'score': comment.score,\n",
        "                'created_utc': comment.created_utc,\n",
        "                'parent_id': comment.parent_id,\n",
        "                'comment_id': comment.id,\n",
        "                'is_root': comment.parent_id == post.id,\n",
        "                'features': comment_features,\n",
        "                'emotion': comment_emotion,\n",
        "                'arg_units': comment_args\n",
        "            })\n",
        "        # metrics used in persuasion score\n",
        "        post_clean = clean_text(post.selftext)\n",
        "        post_features = analyze_persuasive_features(post_clean)\n",
        "        post_emotion = get_enhanced_emotion_scores(post_clean)\n",
        "        post_args = spacy_sent_tokenize(post_clean)\n",
        "        actual_deltas = extract_actual_deltas(post_comments)\n",
        "        persuasion_score = compute_enhanced_persuasion_score(\n",
        "            post_clean, post_comments, actual_deltas\n",
        "        )\n",
        "\n",
        "        # additional metrics\n",
        "        sophistication_score = detect_argument_sophistication(post_clean)\n",
        "        evidence_quality = analyze_source_credibility(post_clean)\n",
        "        engagement_score = analyze_comment_engagement(post_comments)\n",
        "\n",
        "        # Create clean post record\n",
        "        post_data = {\n",
        "            # Basic post info\n",
        "            'post_id': post.id,\n",
        "            'title': post.title,\n",
        "            'author': str(post.author),\n",
        "            'post_text': post_clean,  # Full cleaned text\n",
        "            'original_post_text': post.selftext,  # Original for reference\n",
        "            'created_timestamp': format_timestamp(post.created_utc),\n",
        "            'created_utc': post.created_utc,\n",
        "            'post_score': post.score,\n",
        "            'num_comments': post.num_comments,\n",
        "            'post_url': f\"https://reddit.com{post.permalink}\",\n",
        "\n",
        "            # Text metrics\n",
        "            'post_word_count': len(post_clean.split()),\n",
        "            'post_sentence_count': len(post_args),\n",
        "            'post_character_count': len(post_clean),\n",
        "\n",
        "            # Persuasion scores\n",
        "            'persuasion_score': persuasion_score,\n",
        "            'argument_sophistication_score': sophistication_score,\n",
        "            'evidence_quality_score': evidence_quality,\n",
        "            'comment_engagement_score': engagement_score,\n",
        "            'delta_count': actual_deltas,\n",
        "            'has_deltas': actual_deltas > 0,\n",
        "\n",
        "            # Persuasive features (flattened from post_features)\n",
        "            'analogies_count': post_features.get('analogies', 0),\n",
        "            'questions_count': post_features.get('questions', 0),\n",
        "            'statistics_count': post_features.get('statistics', 0),\n",
        "            'hedging_count': post_features.get('hedging', 0),\n",
        "            'personal_experience_count': post_features.get('personal_experience', 0),\n",
        "            'moral_appeals_count': post_features.get('moral_appeals', 0),\n",
        "            'emotional_appeals_count': post_features.get('emotional_appeals', 0),\n",
        "            'authority_appeals_count': post_features.get('authority_appeals', 0),\n",
        "            'consensus_appeals_count': post_features.get('consensus_appeals', 0),\n",
        "            'concessions_count': post_features.get('concessions', 0),\n",
        "\n",
        "            # Boolean flags for easy filtering\n",
        "            'has_analogies': post_features.get('analogies', 0) > 0,\n",
        "            'has_statistics': post_features.get('statistics', 0) > 0,\n",
        "            'has_personal_experience': post_features.get('personal_experience', 0) > 0,\n",
        "            'has_moral_appeals': post_features.get('moral_appeals', 0) > 0,\n",
        "            'has_authority_appeals': post_features.get('authority_appeals', 0) > 0,\n",
        "\n",
        "            # Sentiment scores (flattened from post_emotion)\n",
        "            'vader_compound': post_emotion['vader_compound'],\n",
        "            'vader_positive': post_emotion['vader_pos'],\n",
        "            'vader_negative': post_emotion['vader_neg'],\n",
        "            'vader_neutral': post_emotion['vader_neu'],\n",
        "            'persuasive_emotion_score': post_emotion['persuasive_emotion'],\n",
        "\n",
        "            # Engagement metrics\n",
        "            'root_comments_count': len([c for c in post_comments if c.get('is_root', False)]),\n",
        "            'reply_comments_count': len([c for c in post_comments if not c.get('is_root', False)]),\n",
        "            'avg_comment_length': np.mean([len(c.get('body', '')) for c in post_comments]) if post_comments else 0,\n",
        "            'total_comment_words': sum([len(c.get('body', '').split()) for c in post_comments])\n",
        "        }\n",
        "\n",
        "        posts_data.append(post_data)\n",
        "\n",
        "        # Brief pause to avoid rate limiting\n",
        "        time.sleep(0.5)\n",
        "\n",
        "    print(f\" Data collection complete. Collected {len(posts_data)} posts and {len(comments_data)} comments\")\n",
        "    return posts_data, comments_data\n",
        "\n",
        "# Enhanced analysis + export functions\n",
        "def save_clean_datasets(posts_data, comments_data):\n",
        "    \"\"\"Save multiple clean CSV files for different analysis needs\"\"\"\n",
        "\n",
        "    # 1. Main posts dataset\n",
        "    posts_df = pd.DataFrame(posts_data)\n",
        "    posts_df.to_csv('cmv_posts_analysis.csv', index=False)\n",
        "    print(f\" Posts data saved to 'cmv_posts_analysis.csv' ({len(posts_df)} rows)\")\n",
        "\n",
        "    # 2. Comments dataset (if we have comment data)\n",
        "    if comments_data:\n",
        "        comments_df = pd.DataFrame(comments_data)\n",
        "        comments_df.to_csv('cmv_comments_analysis.csv', index=False)\n",
        "        print(f\" Comments data saved to 'cmv_comments_analysis.csv' ({len(comments_df)} rows)\")\n",
        "\n",
        "    # 3. Summary statistics dataset\n",
        "    summary_stats = create_summary_statistics(posts_df)\n",
        "    summary_df = pd.DataFrame([summary_stats])\n",
        "    summary_df.to_csv('cmv_summary_stats.csv', index=False)\n",
        "    print(f\" Summary statistics saved to 'cmv_summary_stats.csv'\")\n",
        "\n",
        "    # 4. Top performers dataset (for easy reference)\n",
        "    top_performers = posts_df.nlargest(20, 'persuasion_score')[\n",
        "        ['post_id', 'title', 'persuasion_score', 'delta_count', 'post_score',\n",
        "         'post_word_count', 'has_deltas', 'created_timestamp']\n",
        "    ].copy()\n",
        "    top_performers['rank'] = range(1, len(top_performers) + 1)\n",
        "    top_performers.to_csv('cmv_top_performers.csv', index=False)\n",
        "    print(f\" Top performers saved to 'cmv_top_performers.csv' ({len(top_performers)} rows)\")\n",
        "\n",
        "    return posts_df, comments_df if comments_data else None, summary_df\n",
        "\n",
        "def create_summary_statistics(df):\n",
        "    \"\"\"Create summary stats\"\"\"\n",
        "    return {\n",
        "        'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'total_posts_analyzed': len(df),\n",
        "        'avg_persuasion_score': round(df['persuasion_score'].mean(), 3),\n",
        "        'median_persuasion_score': round(df['persuasion_score'].median(), 3),\n",
        "        'std_persuasion_score': round(df['persuasion_score'].std(), 3),\n",
        "        'max_persuasion_score': round(df['persuasion_score'].max(), 3),\n",
        "        'min_persuasion_score': round(df['persuasion_score'].min(), 3),\n",
        "\n",
        "        'posts_with_deltas': int((df['delta_count'] > 0).sum()),\n",
        "        'posts_with_deltas_percent': round((df['delta_count'] > 0).mean() * 100, 1),\n",
        "        'total_deltas_awarded': int(df['delta_count'].sum()),\n",
        "        'avg_deltas_per_post': round(df['delta_count'].mean(), 2),\n",
        "        'max_deltas_single_post': int(df['delta_count'].max()),\n",
        "\n",
        "        'avg_post_word_count': round(df['post_word_count'].mean(), 0),\n",
        "        'avg_post_sentence_count': round(df['post_sentence_count'].mean(), 1),\n",
        "        'avg_reddit_score': round(df['post_score'].mean(), 1),\n",
        "        'avg_comment_count': round(df['num_comments'].mean(), 1),\n",
        "\n",
        "        # Persuasive technique prevalence\n",
        "        'posts_with_analogies_percent': round((df['has_analogies']).mean() * 100, 1),\n",
        "        'posts_with_statistics_percent': round((df['has_statistics']).mean() * 100, 1),\n",
        "        'posts_with_personal_experience_percent': round((df['has_personal_experience']).mean() * 100, 1),\n",
        "        'posts_with_moral_appeals_percent': round((df['has_moral_appeals']).mean() * 100, 1),\n",
        "        'posts_with_authority_appeals_percent': round((df['has_authority_appeals']).mean() * 100, 1),\n",
        "\n",
        "        # Quality metrics\n",
        "        'avg_argument_sophistication': round(df['argument_sophistication_score'].mean(), 3),\n",
        "        'avg_evidence_quality': round(df['evidence_quality_score'].mean(), 3),\n",
        "        'avg_comment_engagement': round(df['comment_engagement_score'].mean(), 3),\n",
        "\n",
        "        # Sentiment distribution\n",
        "        'avg_vader_compound': round(df['vader_compound'].mean(), 3),\n",
        "        'positive_sentiment_posts_percent': round((df['vader_compound'] > 0.1).mean() * 100, 1),\n",
        "        'negative_sentiment_posts_percent': round((df['vader_compound'] < -0.1).mean() * 100, 1),\n",
        "        'neutral_sentiment_posts_percent': round((abs(df['vader_compound']) <= 0.1).mean() * 100, 1)\n",
        "    }\n",
        "\n",
        "def display_enhanced_analysis_summary(posts_df, comments_df=None):\n",
        "    \"\"\"Enhanced summary with actionable insights\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\" PERSUASION ANALYSIS SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Basic stats\n",
        "    print(f\" Dataset Overview:\")\n",
        "    print(f\"   • Total posts analyzed: {len(posts_df):,}\")\n",
        "    if comments_df is not None:\n",
        "        print(f\"   • Total comments analyzed: {len(comments_df):,}\")\n",
        "    print(f\"   • Analysis completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    # Persuasion metrics\n",
        "    print(f\"\\n Persuasion Metrics:\")\n",
        "    print(f\"   • Average persuasion score: {posts_df['persuasion_score'].mean():.3f}\")\n",
        "    print(f\"   • Highest persuasion score: {posts_df['persuasion_score'].max():.3f}\")\n",
        "    print(f\"   • Posts with deltas: {(posts_df['delta_count'] > 0).sum()} ({(posts_df['delta_count'] > 0).mean()*100:.1f}%)\")\n",
        "    print(f\"   • Total deltas awarded: {posts_df['delta_count'].sum()}\")\n",
        "    print(f\"   • Average deltas per post: {posts_df['delta_count'].mean():.2f}\")\n",
        "\n",
        "    # Content analysis\n",
        "    print(f\"\\n Content Analysis:\")\n",
        "    print(f\"   • Average word count: {posts_df['post_word_count'].mean():.0f}\")\n",
        "    print(f\"   • Average sentence count: {posts_df['post_sentence_count'].mean():.1f}\")\n",
        "    print(f\"   • Posts with statistics: {posts_df['has_statistics'].mean()*100:.1f}%\")\n",
        "    print(f\"   • Posts with personal experience: {posts_df['has_personal_experience'].mean()*100:.1f}%\")\n",
        "    print(f\"   • Posts with moral appeals: {posts_df['has_moral_appeals'].mean()*100:.1f}%\")\n",
        "\n",
        "    # Quality indicators\n",
        "    print(f\"\\n Quality Indicators:\")\n",
        "    print(f\"   • Average argument sophistication: {posts_df['argument_sophistication_score'].mean():.3f}\")\n",
        "    print(f\"   • Average evidence quality: {posts_df['evidence_quality_score'].mean():.3f}\")\n",
        "    print(f\"   • Average comment engagement: {posts_df['comment_engagement_score'].mean():.3f}\")\n",
        "\n",
        "    # Top performing posts\n",
        "    print(f\"\\n Most Persuasive Posts:\")\n",
        "    top_posts = posts_df.nlargest(5, 'persuasion_score')[\n",
        "        ['title', 'persuasion_score', 'delta_count', 'post_score']\n",
        "    ]\n",
        "\n",
        "    for i, (_, row) in enumerate(top_posts.iterrows(), 1):\n",
        "        title_truncated = row['title'][:55] + \"...\" if len(row['title']) > 55 else row['title']\n",
        "        print(f\"   {i}. [{row['persuasion_score']:.3f}] {title_truncated}\")\n",
        "        print(f\"      Deltas: {row['delta_count']} | Reddit Score: {row['post_score']}\")\n",
        "\n",
        "    # Correlation insights\n",
        "    print(f\"\\n Key Correlations:\")\n",
        "    corr_deltas = posts_df['persuasion_score'].corr(posts_df['delta_count'])\n",
        "    corr_reddit_score = posts_df['persuasion_score'].corr(posts_df['post_score'])\n",
        "    corr_word_count = posts_df['persuasion_score'].corr(posts_df['post_word_count'])\n",
        "\n",
        "    print(f\"   • Persuasion score ↔ Delta count: {corr_deltas:.3f}\")\n",
        "    print(f\"   • Persuasion score ↔ Reddit score: {corr_reddit_score:.3f}\")\n",
        "    print(f\"   • Persuasion score ↔ Word count: {corr_word_count:.3f}\")\n",
        "\n",
        "    print(f\"\\n Files Generated:\")\n",
        "    print(f\"   • cmv_posts_analysis.csv - Main dataset with all post metrics\")\n",
        "    if comments_df is not None:\n",
        "        print(f\"   • cmv_comments_analysis.csv - Individual comment analysis\")\n",
        "    print(f\"   • cmv_summary_stats.csv - Aggregated statistics\")\n",
        "    print(f\"   • cmv_top_performers.csv - Top 20 most persuasive posts\")\n",
        "\n",
        "def create_data_dictionary():\n",
        "    \"\"\"Generate a data dictionary for the CSV files\"\"\"\n",
        "\n",
        "    posts_dictionary = {\n",
        "        'Column Name': [\n",
        "            'post_id', 'title', 'author', 'post_text', 'original_post_text',\n",
        "            'created_timestamp', 'created_utc', 'post_score', 'num_comments',\n",
        "            'post_url', 'post_word_count', 'post_sentence_count', 'post_character_count',\n",
        "            'persuasion_score', 'argument_sophistication_score', 'evidence_quality_score',\n",
        "            'comment_engagement_score', 'delta_count', 'has_deltas',\n",
        "            'analogies_count', 'questions_count', 'statistics_count', 'hedging_count',\n",
        "            'personal_experience_count', 'moral_appeals_count', 'emotional_appeals_count',\n",
        "            'authority_appeals_count', 'consensus_appeals_count', 'concessions_count',\n",
        "            'has_analogies', 'has_statistics', 'has_personal_experience',\n",
        "            'has_moral_appeals', 'has_authority_appeals',\n",
        "            'vader_compound', 'vader_positive', 'vader_negative', 'vader_neutral',\n",
        "            'persuasive_emotion_score', 'root_comments_count', 'reply_comments_count',\n",
        "            'avg_comment_length', 'total_comment_words'\n",
        "        ],\n",
        "        'Description': [\n",
        "            'Unique Reddit post identifier',\n",
        "            'Post title text',\n",
        "            'Reddit username of post author',\n",
        "            'Cleaned post content text',\n",
        "            'Original unprocessed post text',\n",
        "            'Human-readable creation timestamp',\n",
        "            'Unix timestamp of post creation',\n",
        "            'Reddit upvote score',\n",
        "            'Total number of comments',\n",
        "            'Direct URL to Reddit post',\n",
        "            'Number of words in post',\n",
        "            'Number of sentences in post',\n",
        "            'Total character count',\n",
        "            'Overall persuasion score (0-1)',\n",
        "            'Argument sophistication score (0-1)',\n",
        "            'Quality of cited evidence (0-1)',\n",
        "            'Comment engagement quality (0-1)',\n",
        "            'Number of delta awards received',\n",
        "            'Boolean: post received any deltas',\n",
        "            'Count of analogies used',\n",
        "            'Count of questions asked',\n",
        "            'Count of statistics/numbers cited',\n",
        "            'Count of hedging language used',\n",
        "            'Count of personal experience references',\n",
        "            'Count of moral/ethical appeals',\n",
        "            'Count of emotional language',\n",
        "            'Count of authority citations',\n",
        "            'Count of consensus appeals',\n",
        "            'Count of concessions made',\n",
        "            'Boolean: contains analogies',\n",
        "            'Boolean: contains statistics',\n",
        "            'Boolean: contains personal experience',\n",
        "            'Boolean: contains moral appeals',\n",
        "            'Boolean: contains authority appeals',\n",
        "            'VADER sentiment compound score (-1 to 1)',\n",
        "            'VADER positive sentiment (0-1)',\n",
        "            'VADER negative sentiment (0-1)',\n",
        "            'VADER neutral sentiment (0-1)',\n",
        "            'Domain-specific persuasive emotion score',\n",
        "            'Number of top-level comments',\n",
        "            'Number of reply comments',\n",
        "            'Average length of comments',\n",
        "            'Total words across all comments'\n",
        "        ],\n",
        "        'Data Type': [\n",
        "            'string', 'string', 'string', 'string', 'string',\n",
        "            'datetime', 'integer', 'integer', 'integer', 'string',\n",
        "            'integer', 'integer', 'integer', 'float', 'float', 'float',\n",
        "            'float', 'integer', 'boolean', 'integer', 'integer', 'integer',\n",
        "            'integer', 'integer', 'integer', 'integer', 'integer', 'integer',\n",
        "            'integer', 'boolean', 'boolean', 'boolean', 'boolean', 'boolean',\n",
        "            'float', 'float', 'float', 'float', 'float', 'integer', 'integer',\n",
        "            'float', 'integer'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    dictionary_df = pd.DataFrame(posts_dictionary)\n",
        "    dictionary_df.to_csv('cmv_data_dictionary.csv', index=False)\n",
        "    print(f\" Data dictionary saved to 'cmv_data_dictionary.csv'\")\n",
        "\n",
        "    return dictionary_df\n",
        "\n",
        "# execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Collect data\n",
        "    posts_data, comments_data = collect_cmv_data(limit=3, time_filter=\"year\")\n",
        "\n",
        "    # Save clean datasets\n",
        "    posts_df, comments_df, summary_df = save_clean_datasets(posts_data, comments_data)\n",
        "\n",
        "    # Create data dictionary\n",
        "    dictionary_df = create_data_dictionary()\n",
        "\n",
        "    # Display comprehensive summary\n",
        "    display_enhanced_analysis_summary(posts_df, comments_df)\n",
        "\n",
        "    print(f\"\\n Analysis complete.\")\n",
        "    print(f\" Check 'cmv_data_dictionary.csv' for column explanations.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3j6QSfL-ge2",
        "outputId": "182f6949-f0bb-43f0-eb49-a08af9da3f30",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Starting enhanced CMV data collection (limit: 3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Processing post 1/3: CMV: Anyone who votes for Trump is completely lack...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Processing post 2/3: CMV: Voting for Donald Trump in the 2024 election ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Processing post 3/3: CMV: The online left has failed young men...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Data collection complete. Collected 3 posts and 2602 comments\n",
            " Posts data saved to 'cmv_posts_analysis.csv' (3 rows)\n",
            " Comments data saved to 'cmv_comments_analysis.csv' (2602 rows)\n",
            " Summary statistics saved to 'cmv_summary_stats.csv'\n",
            " Top performers saved to 'cmv_top_performers.csv' (3 rows)\n",
            " Data dictionary saved to 'cmv_data_dictionary.csv'\n",
            "\n",
            "============================================================\n",
            " PERSUASION ANALYSIS SUMMARY\n",
            "============================================================\n",
            " Dataset Overview:\n",
            "   • Total posts analyzed: 3\n",
            "   • Total comments analyzed: 2,602\n",
            "   • Analysis completed: 2025-09-02 03:54:19\n",
            "\n",
            " Persuasion Metrics:\n",
            "   • Average persuasion score: 0.472\n",
            "   • Highest persuasion score: 0.531\n",
            "   • Posts with deltas: 3 (100.0%)\n",
            "   • Total deltas awarded: 11\n",
            "   • Average deltas per post: 3.67\n",
            "\n",
            " Content Analysis:\n",
            "   • Average word count: 572\n",
            "   • Average sentence count: 26.7\n",
            "   • Posts with statistics: 100.0%\n",
            "   • Posts with personal experience: 66.7%\n",
            "   • Posts with moral appeals: 100.0%\n",
            "\n",
            " Quality Indicators:\n",
            "   • Average argument sophistication: 0.163\n",
            "   • Average evidence quality: 0.000\n",
            "   • Average comment engagement: 0.933\n",
            "\n",
            " Most Persuasive Posts:\n",
            "   1. [0.531] CMV: The online left has failed young men\n",
            "      Deltas: 3 | Reddit Score: 5389\n",
            "   2. [0.473] CMV: Anyone who votes for Trump is completely lacking i...\n",
            "      Deltas: 3 | Reddit Score: 8745\n",
            "   3. [0.413] CMV: Voting for Donald Trump in the 2024 election means...\n",
            "      Deltas: 5 | Reddit Score: 6337\n",
            "\n",
            " Key Correlations:\n",
            "   • Persuasion score ↔ Delta count: -0.871\n",
            "   • Persuasion score ↔ Reddit score: -0.265\n",
            "   • Persuasion score ↔ Word count: 0.982\n",
            "\n",
            " Files Generated:\n",
            "   • cmv_posts_analysis.csv - Main dataset with all post metrics\n",
            "   • cmv_comments_analysis.csv - Individual comment analysis\n",
            "   • cmv_summary_stats.csv - Aggregated statistics\n",
            "   • cmv_top_performers.csv - Top 20 most persuasive posts\n",
            "\n",
            " Analysis complete.\n",
            " Check 'cmv_data_dictionary.csv' for column explanations.\n"
          ]
        }
      ]
    }
  ]
}